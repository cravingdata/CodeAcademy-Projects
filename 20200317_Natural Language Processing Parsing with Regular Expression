#Set 1:
import re

# characters are defined
character_1 = "Dorothy"
character_2 = "Henry"

# compile your regular expression here
regular_expression = re.compile("[A-Za-z]{7}")

# check for a match to character_1 here
result_1 = regular_expression.match(character_1)
match_1 = result_1.group(0)

# store and print the matched text here
print(match_1)

# compile a regular expression to match a 7 character string of word characters and check for a match to character_2 here
result_2 = re.match("[A-Za-z]{7}", character_2)
print(result_2)


#Set 2
import re

# import L. Frank Baum's The Wonderful Wizard of Oz
oz_text = open("the_wizard_of_oz_text.txt",encoding='utf-8').read().lower()

# search oz_text for an occurrence of 'wizard' here
found_wizard = re.search("wizard", oz_text)
print(found_wizard)

# find all the occurrences of 'lion' in oz_text here
all_lions = re.findall("lion", oz_text)
print(all_lions)

# store and print the length of all_lions here
number_lions = len(all_lions)
print(number_lions)


#Set 3
from nltk import RegexpParser, Tree
from pos_tagged_oz import pos_tagged_oz

# define adjective-noun chunk grammar here
chunk_grammar = "AN: {<JJ><NN>}"

# create RegexpParser object here
chunk_parser = RegexpParser(chunk_grammar)

# chunk the pos-tagged sentence at index 282 in pos_tagged_oz here
scaredy_cat = chunk_parser.parse(pos_tagged_oz[282])
print(scaredy_cat)

# pretty_print the chunked sentence here
Tree.fromstring(str(scaredy_cat)).pretty_print()

#Set 4
import nltk
from nltk import pos_tag
from word_tokenized_oz import word_tokenized_oz

# save and print the sentence stored at index 100 in word_tokenized_oz here
print(word_tokenized_oz)
witches_fate = word_tokenized_oz[100]
print(witches_fate)
"""This sample here shows us what the text at index 100 looks like without it being tagged."""

# create a list to hold part-of-speech tagged sentences here
pos_tagged_oz = list()

# create a for loop through each word tokenized sentence in word_tokenized_oz here
for word_tokenized_sentence in word_tokenized_oz:
  # part-of-speech tag each sentence and append to pos_tagged_oz here
  pos_tagged_oz.append(pos_tag(word_tokenized_sentence))
  
"""Here, the for loop allows all of the words in word_tokenized_oz to be appended into a list."""
print(pos_tagged_oz)
# store and print the part-of-speech tagged sentence at index 100 in witches_fate_pos here
witches_fate_pos = pos_tagged_oz[100]
print(witches_fate_pos)

#Set 5
from nltk import RegexpParser
from pos_tagged_oz import pos_tagged_oz
from np_chunk_counter import np_chunk_counter

# define noun-phrase chunk grammar here
chunk_grammar = "NP: {<DT>?<JJ>*<NN>}"

"""NP is the user-defined name of the chunk you are searching for. In this case NP stands for noun phrase
<DT> matches any determiner
? is an optional quantifier, matching either 0 or 1 determiners
<JJ> matches any adjective
* is the Kleene star quantifier, matching 0 or more occurrences of an adjective
<NN> matches any noun, singular or plural"""
# create RegexpParser object here
chunk_parser = RegexpParser(chunk_grammar)

# create a list to hold noun-phrase chunked sentences
np_chunked_oz = list()

# create a for loop through each pos-tagged sentence in pos_tagged_oz here
for word_pos_tagged_oz in pos_tagged_oz:
  
  # chunk each sentence and append to np_chunked_oz here
  np_chunked_oz.append(chunk_parser.parse(word_pos_tagged_oz))

# store and print the most common np-chunks here
most_common_np_chunks = np_chunk_counter(np_chunked_oz)
print(most_common_np_chunks)


